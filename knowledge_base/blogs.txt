ğŸ“„ Blog Post â€“ DEV Community

Title: Sentiment Analysis, the Classical Way â€” No Deep Learning
Publisher: DEV Community
Author: Anik Chand
Date Published: Aug 6, 2025
URL: https://dev.to/anikchand461/sentiment-analysis-the-classical-way-no-deep-learning-3lc6

â¸»

Description (for LinkedIn post/entry):

â€œCan you get high accuracy in sentiment analysis without deep learning?â€
Thatâ€™s the question I asked myself â€” and the results surprised me.

In this blog, I shared how I built a sentiment analysis system using only classical Machine Learning algorithms (no RNNs, LSTMs, or BERT!) and still reached an accuracy of 89.12%.

ğŸ”¹ Dataset: IMDb reviews (40k train, 10k test)
ğŸ”¹ Models Tried: Logistic Regression, LinearSVC, SGDClassifier, Naive Bayes, PassiveAggressive
ğŸ”¹ Best Result: 1-layer Stacking Ensemble â†’ 89.12% Accuracy
ğŸ”¹ Key Tools: Scikit-learn, NLTK, Pandas, Matplotlib

ğŸ’¡ Key Takeaways:
	â€¢	Traditional ML still holds strong in NLP tasks
	â€¢	Logistic Regression + TF-IDF is surprisingly powerful
	â€¢	Ensemble methods like stacking can outperform individual models




ğŸ“„ Blog Post â€“ DEV Community

Title: The Curve That Judges Your ML Model (AUC-ROC Explained)
Publisher: DEV Community
Author: Anik Chand
Date Published: Jul 13, 2025
URL: https://dev.to/anikchand461/understanding-the-auc-roc-curve-in-machine-learning-with-python-code-3nlc

â¸»

Description (for LinkedIn/Portfolio):

Ever felt proud of your modelâ€™s 95% accuracy, only to realize itâ€™s misleading? Thatâ€™s where the AUC-ROC curve comes in.

In this blog, I explained ROC curves and AUC in the simplest possible way â€” with real-world examples, spam detection cases, and Python code.

ğŸ”¹ Why Accuracy Fails: Rare diseases, spam detection, churn prediction â€” accuracy isnâ€™t enough.
ğŸ”¹ What ROC Shows: Trade-off between True Positives and False Positives at different thresholds.
ğŸ”¹ What AUC Means: Overall model performance across thresholds (1.0 = perfect, 0.5 = random).
ğŸ”¹ Code Demo: RandomForestClassifier with roc_curve and roc_auc_score.
ğŸ”¹ Key Learnings:
	â€¢	Accuracy isnâ€™t always reliable
	â€¢	ROC curve = visual skill check
	â€¢	AUC = model ranking ability
	â€¢	Thresholds matter for business impact

ğŸ“Œ By the end, youâ€™ll understand how to read, interpret, and implement AUC-ROC in your own ML projects.

â¸»

ğŸ‘‰ Repo/Notebook can also be added if you have one.



ğŸ“„ Blog Post â€“ DEV Community

Title: ğŸš¢ OneHotEncoder Shape Mismatch Mystery in Titanic Dataset â€” Solved!
Publisher: DEV Community
Author: Anik Chand
Date Published: Apr 7, 2025 (Edited Jul 13, 2025)
URL: https://dev.to/anikchand461/onehotencoder-shape-mismatch-mystery-in-titanic-dataset-solved-3n6

â¸»

Description (for LinkedIn/Portfolio):

While working on the Titanic dataset (CampusX course), I ran into a tricky issue:

My OneHotEncoder gave different shapes for train and test sets â€” (712, 4) vs (179, 3). ğŸ¤¯

After debugging, I realized:
	â€¢	Missing values were not assigned back to the original DataFrame after SimpleImputer
	â€¢	Encoder treated NaN as a new category, creating extra columns in train data
	â€¢	Test data had no NaN â†’ fewer categories â†’ shape mismatch

âœ… Fix: Always impute back into your DataFrame before encoding.

ğŸ“Œ Key Learnings:
	â€¢	Always handle missing values before encoding
	â€¢	handle_unknown='ignore' avoids errors, but doesnâ€™t fix mismatched categories
	â€¢	Careful order of preprocessing steps = stable pipeline

This debugging session was a great reminder that data preprocessing mistakes can silently break ML models. Hopefully, it saves someone else hours of confusion!




ğŸ“Œ Regression in ML â€” Knowledge Notes
url - https://dev.to/anikchand461/regression-in-ml-explained-the-ultimate-hands-on-guide-484f
Posted on Mar 19, 2025

ğŸ”¹ What is Regression?
	â€¢	Regression = Predicting continuous values (numbers).
	â€¢	Examples: House prices ğŸ¡, stock trends ğŸ“ˆ, sales forecasting ğŸ›ï¸, weather ğŸŒ¦ï¸.

â¸»

1ï¸âƒ£ Linear Regression
	â€¢	Assumes a straight-line relation between X and Y.
	â€¢	Formula: Y = bâ‚€ + bâ‚X + Îµ
	â€¢	âœ… Uses: stock prices, sales, real estate, medical risk analysis.
	â€¢	âš ï¸ Check assumptions: linearity, independence, residuals.

â¸»

2ï¸âƒ£ Multiple Linear Regression
	â€¢	Predicts using multiple features.
	â€¢	Formula: Y = bâ‚€ + bâ‚Xâ‚ + bâ‚‚Xâ‚‚ + â€¦ + bâ‚™Xâ‚™ + Îµ
	â€¢	âœ… Uses: house pricing (size, bedrooms, location).
	â€¢	âš ï¸ Challenges: overfitting, feature selection, normalization.

â¸»

3ï¸âƒ£ Polynomial Regression
	â€¢	Captures non-linear (curved) data.
	â€¢	Formula: Y = bâ‚€ + bâ‚X + bâ‚‚XÂ² + â€¦ + bâ‚™Xâ¿ + Îµ
	â€¢	âœ… Uses: salary growth, COVID-19 trends, vehicle performance.
	â€¢	âš ï¸ Risk of overfitting with high degree.

â¸»

4ï¸âƒ£ Logistic Regression (Classification)
	â€¢	Despite name â†’ used for classification (Yes/No).
	â€¢	Formula: P = 1 / (1 + e^(-z)) (Sigmoid function).
	â€¢	âœ… Uses: spam detection, pass/fail, customer conversion.

â¸»




another blog ...



ğŸ® Hangman Game â€” Knowledge Notes

url - https://dev.to/anikchand461/crafting-fun-with-code-my-journey-building-the-hangman-game-11ih
posted on Jan 7, 2025

ğŸ“Œ Motivation
	â€¢	First blog inspired by friend Abhiraj Adhikary.
	â€¢	Purpose: Learn Python + share knowledge with community.

â¸»

ğŸ•¹ï¸ What is Hangman?
	â€¢	Classic word-guessing game.
	â€¢	Guess letters â†’ incorrect guesses draw parts of stick figure.
	â€¢	Tests vocabulary, logic, and problem-solving.

â¸»

ğŸ¯ Objective
	â€¢	Fun game with categories (sports, food, animals).
	â€¢	Learn Python concepts: loops, conditionals, random module.
	â€¢	Improve logic-building and coding confidence.

â¸»

âœ¨ Features
	â€¢	Diverse categories (sports, food, animals).
	â€¢	Random word selection â†’ unpredictable gameplay.
	â€¢	ASCII Art â†’ visual Hangman + win/lose graphics.
	â€¢	Replay option â†’ multiple rounds.
	â€¢	User-friendly â†’ error handling, score keeping.
	â€¢	Difficulty Levels: easy (1â€“5 letters), medium (5â€“7), hard (8+).

â¸»

ğŸ› ï¸ Tech Stack
	â€¢	Language: Python ğŸ
	â€¢	Libraries:
	â€¢	random â†’ select words
	â€¢	os â†’ clear screen between turns

â¸»

ğŸ”„ Development Stages
	1.	Basic Version
	â€¢	Predefined word, dashes, letter replacement.
	â€¢	Player wins if word guessed, else game over.
	2.	Improvements
	â€¢	Replace all occurrences of guessed letters.
	â€¢	Lives system (6 attempts).
	3.	Enhancements
	â€¢	Categories (sports, food, animals).
	â€¢	Difficulty levels.
	â€¢	ASCII art for progress + endings.
	â€¢	Replay loop + game summary.

â¸»

ğŸ“š Lessons Learned
	1.	Problem-Solving â†’ Debugging errors, handling invalid inputs, edge cases.
	2.	Time Management â†’ Balancing learning + coding, prioritizing features.

â¸»

ğŸ”— Repo

ğŸ‘‰ GitHub: Hangman Game


